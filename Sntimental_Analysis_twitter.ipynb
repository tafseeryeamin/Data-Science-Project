{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "JCBily-gnqJh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b68a006d-fdf0-4b20-eb98-b10b127f0588"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.46.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.26.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.6)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tweepy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lIUjJQb9hs1O",
        "outputId": "533dfe27-c23c-4efc-c2e6-73cb49253077"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tweepy in /usr/local/lib/python3.10/dist-packages (4.14.0)\n",
            "Requirement already satisfied: oauthlib<4,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from tweepy) (3.2.2)\n",
            "Requirement already satisfied: requests<3,>=2.27.0 in /usr/local/lib/python3.10/dist-packages (from tweepy) (2.32.3)\n",
            "Requirement already satisfied: requests-oauthlib<2,>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from tweepy) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27.0->tweepy) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27.0->tweepy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27.0->tweepy) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27.0->tweepy) (2024.8.30)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "from scipy.special import softmax"
      ],
      "metadata": {
        "id": "fqVaykWeoNxK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tweepy\n",
        "import time\n",
        "\n",
        "# Set up keys\n",
        "\n",
        "\n",
        "# Authenticate\n",
        "client = tweepy.Client(bearer_token=bearer_token, wait_on_rate_limit=True)  # Added wait_on_rate_limit=True\n",
        "\n",
        "# Search recent tweets\n",
        "query = \"elon musk\"\n",
        "response = client.search_recent_tweets(query=query, max_results=10)\n",
        "\n",
        "# List to store tweet data\n",
        "tweets_data = []\n",
        "\n",
        "# Process each tweet\n",
        "for tweet in response.data:\n",
        "    tweet_info = {\n",
        "        \"id\": tweet.id,\n",
        "        \"text\": tweet.text,\n",
        "        \"created_at\": tweet.created_at,\n",
        "        \"author_id\": tweet.author_id\n",
        "    }\n",
        "    tweets_data.append(tweet_info)\n",
        "    time.sleep(1)  # Added a 1-second delay between tweets\n",
        "\n",
        "# Now `tweets_data` is a list containing the tweet information\n",
        "print(tweets_data)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2jIeTmxOnJWi",
        "outputId": "fe193e0f-05de-4932-ce2e-277254dac475"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tweepy.client:Rate limit exceeded. Sleeping for 628 seconds.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'id': 1858375647112130875, 'text': 'RT @Vivek4real_: 🇺🇸 Elon Musk just recommended bitcoiner Howard Lutnic as US Treasury 🤯\\n\\nHe has 100’s of millions of dollars in #Bitcoin ht…', 'created_at': None, 'author_id': None}, {'id': 1858375646449377365, 'text': 'RT @OliLondonTV: Elon Musk debuts new look. 🤓 https://t.co/UzCS35Plhb', 'created_at': None, 'author_id': None}, {'id': 1858375644662341743, 'text': 'RT @HaberKuresi034: 🛑 Enes Kanter, Trump’ın Ardından Bu Kez Elon Musk ile Bir Araya Geldi!\\n\\n📌 Geçtiğimiz günlerde ABD Başkanı Donald Trump…', 'created_at': None, 'author_id': None}, {'id': 1858375644138336601, 'text': 'RT @solclownworld: @ZssBecker $CLOWN https://t.co/qu4jKm6Wbn CA: DSEofUG1xUoq8hhzrixf2rjvsjGYuLan7jfsLyKsTRfi\\n\\nPopularized by frequent Elon…', 'created_at': None, 'author_id': None}, {'id': 1858375642410029299, 'text': 'elon musk will leave twitter before i do', 'created_at': None, 'author_id': None}, {'id': 1858375641436999970, 'text': '@mjfree Elon musk sold out America by buying the election', 'created_at': None, 'author_id': None}, {'id': 1858375640577155117, 'text': 'RT @Carmenbae90I: Elon Musk has altered the like button for the U.S. elections. Is this true? #Election2024 #Trump #Kamala #FightforAmerica…', 'created_at': None, 'author_id': None}, {'id': 1858375640237629664, 'text': 'RT @RawheaD: イーロン個人に批判的でもSpaceXやStalinkにはかなり肯定的なアカが𝕏には多いが、科学／天文の観点からもStarlinkは壊滅的な環境破壊を行ってる。下手したら近々特定の種類の電波望遠鏡観測が不可能になるかもしれないという研究結果：\\n\\nhttp…', 'created_at': None, 'author_id': None}, {'id': 1858375640124117177, 'text': 'RT @solclownworld: @ZssBecker $CLOWN https://t.co/qu4jKm6Wbn CA: DSEofUG1xUoq8hhzrixf2rjvsjGYuLan7jfsLyKsTRfi\\n\\nPopularized by frequent Elon…', 'created_at': None, 'author_id': None}, {'id': 1858375635321680356, 'text': 'RT @merry123459: The most disturbing part about Doug Fords Starlink deal, isn’t so much about Starlink as it about him doing business with…', 'created_at': None, 'author_id': None}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Key Changes:\n",
        "process_tweet Function: This function processes the tweet text by splitting the text into words and replacing:\n",
        "\n",
        "Mentions (starting with @) with @user.\n",
        "\n",
        "URLs (starting with http) with http.\n",
        "\n",
        "The words are then joined back into a string.\n",
        "\n",
        "Processing Tweets: After extracting each tweet's original text, I call\n",
        "process_tweet() to create a processed version of the tweet's text.\n",
        "\n",
        "Storing Processed Text: Both the original tweet text (text) and the processed text (processed_text) are stored in the tweets_data list.\n",
        "\n"
      ],
      "metadata": {
        "id": "Qf7acRIilMwZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#load the model and tokenizer\n",
        "roberta = 'cardiffnlp/twitter-roberta-base-sentiment'\n",
        "model = AutoModelForSequenceClassification.from_pretrained(roberta)\n",
        "tokenizer = AutoTokenizer.from_pretrained(roberta)"
      ],
      "metadata": {
        "id": "2xZlBplLs7rW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loop through each tweet and print only the 'text' field\n",
        "for tweet in tweets_data:\n",
        "    print(tweet['text'])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NZpyJZHwvihL",
        "outputId": "d4361227-ba48-4659-eb8a-3bd4433dc0b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "@gp_pulipaka Fantastic breakdown of the data science landscape! This visual really captures the breadth of skills and knowledge areas needed. A great guide for aspiring data scientists!\n",
            "Stay tuned to get more such scientific motivation &amp; the latest trends in AI, ML, Deep Learning, Data Science and Robotics🤝\n",
            " \n",
            "#MotivationalMonday #MondayMotivation #Inspiration #Motivation #ProductivityBoost #ScienceMotivation #TechInspiration #Innovation #FutureOfTech #wsro2024 https://t.co/lvEtvDiNtD\n",
            "RT @KrutikaKuppalli: Another bad idea: Amid ongoing and emerging infectious disease threats, nominating RFK and Bhattacharya—figures who sp…\n",
            "RT @KirkDBorne: Learn #Mathematics for Data Science for FREE using these Courses: https://t.co/sd7TRR8Plc… — compiled by @tut_ml\n",
            "————\n",
            "#Line…\n",
            "RT @flipsidecrypto: When blockchains grow, we all benefit. But they don't magically grow on their own.\n",
            "\n",
            "Over the years, through the painsta…\n",
            "RT @flipsidecrypto: When blockchains grow, we all benefit. But they don't magically grow on their own.\n",
            "\n",
            "Over the years, through the painsta…\n",
            "RT @flipsidecrypto: When blockchains grow, we all benefit. But they don't magically grow on their own.\n",
            "\n",
            "Over the years, through the painsta…\n",
            "@kevinnbass I AM SCIENCE. DON'T FORGET DATA TOO.\n",
            "RT @flipsidecrypto: When blockchains grow, we all benefit. But they don't magically grow on their own.\n",
            "\n",
            "Over the years, through the painsta…\n",
            "RT @flipsidecrypto: When blockchains grow, we all benefit. But they don't magically grow on their own.\n",
            "\n",
            "Over the years, through the painsta…\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Initialize tokenizer (assuming a pre-trained tokenizer like BERT)\n",
        "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Loop through each tweet, tokenize the text, and print the encoded version\n",
        "for tweet in tweets_data:\n",
        "    encoded_tweet = tokenizer(tweet['text'], return_tensors='pt')\n",
        "    print(encoded_tweet)\n",
        "    # Pass the tokenized tweet to the model\n",
        "    output = model(**encoded_tweet)\n",
        "\n",
        "    # Print the model output\n",
        "    print(\"The model output is:\",\" \",output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pQobzZbbvtl2",
        "outputId": "0ce9d515-bdd4-4c51-aed8-154ed019d0c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': tensor([[  101, 19387,  1030,  8788,  4213,  7317,  2487,  1024,  2256,  5394,\n",
            "          1997,  1996,  2154,  2003,  1012,  1012,  1012, 11407,  3660,  1010,\n",
            "          1996,  4654,  1011,  2564,  2000,  5076,  2022, 28370,  1012,  2023,\n",
            "          2733,  3449,  2239, 14163,  6711,  2056,  3660,  2001,  9846,  2530,\n",
            "          1529,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
            "The model output is:   SequenceClassifierOutput(loss=None, logits=tensor([[-1.1594,  1.5885, -0.6231]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "{'input_ids': tensor([[  101, 19387,  1030,  2613,  9453,  2595, 14339,  2229,  1024,  7262,\n",
            "           999,  4074,  3557,  4283,  3449,  2239, 14163,  6711,  2005,  1060,\n",
            "         13058,  5738, 18558,  9028,  2015, 10528,  2553,  1998,  3640,  3145,\n",
            "         10651,  2006,  8275,  8740,  6593,  1529,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
            "The model output is:   SequenceClassifierOutput(loss=None, logits=tensor([[-0.3721,  1.6788, -1.5158]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "{'input_ids': tensor([[  101, 14163,  6711,  2758,  2010,  4171,  7659,  2097,  2022,  1037,\n",
            "         26479,  2005,  2116,  2138,  1997,  1996,  3454,  2002,  1005,  2222,\n",
            "          2022,  6276,  1012,  2425,  2033,  1010,  3449,  2239, 14163,  6711,\n",
            "          1010,  2097,  2017,  2022,  6276,  2505,  2008,  2747,  3632,  2000,\n",
            "         22301,  2015,  2066,  4426,  1029,   100, 16770,  1024,  1013,  1013,\n",
            "          1056,  1012,  2522,  1013,  1051,  4183,  4160, 24798,  2140,  4160,\n",
            "          2615,  2232, 16770,  1024,  1013,  1013,  1056,  1012,  2522,  1013,\n",
            "          1038,  5104,  2575,  2475, 11927,  2581,  2595,  2080,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1]])}\n",
            "The model output is:   SequenceClassifierOutput(loss=None, logits=tensor([[ 0.5958,  1.1357, -1.8043]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "{'input_ids': tensor([[  101, 19387,  1030, 11939,  2243,  1024,  3449,  2239, 14163,  6711,\n",
            "          6866,  1037,  4689,  2678,  2055,  2129,  2057,  2031,  2000,  2175,\n",
            "          2083,  2019,  1000, 14446,  2724,  1000,  2000,  2468,  6428,  1012,\n",
            "           102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1]])}\n",
            "The model output is:   SequenceClassifierOutput(loss=None, logits=tensor([[ 0.9720,  1.0735, -2.1785]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "{'input_ids': tensor([[  101, 19387,  1030, 17095,  7159,  2080,  1024,  1000,  1037,  5553,\n",
            "          3900,  6583, 17491,  2063, 16908,  2906,  4012,  1051,  3449,  2239,\n",
            "         14163,  6711, 10975,  2050,  6583,  2080, 13675,  2401,  2099,  2012,\n",
            "         28414,  8041,  2080,  1000,  1011,  8945, 14881, 10464,  1012,  1051,\n",
            "          2033, 25855,  1024,  1011,  2572,  5243,  3597,  2226,  3533,  7226,\n",
            "          2368,  4012,  1052,  1529,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1]])}\n",
            "The model output is:   SequenceClassifierOutput(loss=None, logits=tensor([[ 1.3193,  0.8745, -2.4223]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "{'input_ids': tensor([[  101, 19387,  1030, 20996,  2595,  5302,  1024,  5553,  3900,  2158,\n",
            "          2850, 28667,  9365, 29364, 11498,  3449,  2239, 14163,  6711,  1041,\n",
            "         15699,  2063,  7977, 11610, 24528, 14163,  2140,  5886,  4830,  5127,\n",
            "          2401, 16770,  1024,  1013,  1013,  1056,  1012,  2522,  1013,  1060,\n",
            "          6977,  4160,  4160,  3501,  2475,  2595,  2692,  2140,  1013,  1013,\n",
            "          1041,  8529,  2050,  5477,  2050,  1012,  1012,  1012,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
            "The model output is:   SequenceClassifierOutput(loss=None, logits=tensor([[-0.1779,  1.5222, -1.4470]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "{'input_ids': tensor([[  101, 19387,  1030,  2681,  9247, 10830,  2890,  1024,  8452,  6794,\n",
            "          9416,  3449,  2239, 14163,  6711,  1521,  1055,  3477,  7427,  2750,\n",
            "          2383, 18668,  6226,  1012,  8452,  6794, 24433,  1996,  4190,  2050,\n",
            "          1529,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1]])}\n",
            "The model output is:   SequenceClassifierOutput(loss=None, logits=tensor([[ 0.1611,  1.5932, -1.8470]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "{'input_ids': tensor([[  101, 19387,  1030,  3422,  2121, 27390,  2226,  1024,  2074,  1999,\n",
            "          1024,   100,  6819,  3726,  2243, 14115, 26760, 24079,  2758,  2002,\n",
            "          1998,  3449,  2239, 14163,  6711,  1005,  1055,  2533,  1997,  2231,\n",
            "          8122,  1006,  3899,  2063,  1007,  2097,  3972, 12870,  3056,  2287,\n",
            "         12273,  2666,  1529,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
            "The model output is:   SequenceClassifierOutput(loss=None, logits=tensor([[ 0.4227,  1.1518, -1.5949]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "{'input_ids': tensor([[  101, 19387,  1030,  3062,  5400, 25509,  2906,  1024, 14764,  1012,\n",
            "          5924,  7288,  2043,  2009,  2003,  2051,  2000,  2831,  1012,  2025,\n",
            "         20514,  8040, 28094,  1012,  2025, 13489, 19607,  2319,  1012,  2025,\n",
            "          6221,  8398,  1012,  1998,  5791,  1529,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
            "The model output is:   SequenceClassifierOutput(loss=None, logits=tensor([[ 0.1579,  1.1841, -1.4964]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "{'input_ids': tensor([[  101, 19387,  1030, 11320, 22117, 11733, 16523, 22360, 14526,  1024,\n",
            "          5553,  3900,  1010,  3449,  2239, 14163,  6711,  1041,  1037, 17491,\n",
            "         27360,  2050,  1041, 15549,  5051,  2139,  8398, 16770,  1024,  1013,\n",
            "          1013,  1056,  1012,  2522,  1013, 16420, 12881,  2615,  2509,  7898,\n",
            "          2595,  2243,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
            "The model output is:   SequenceClassifierOutput(loss=None, logits=tensor([[-0.8979,  1.8633, -1.1348]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "labels = ['Negative', 'Neutral', 'Positive']"
      ],
      "metadata": {
        "id": "GJaQLB5atbnG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn import functional as F\n",
        "import torch\n",
        "\n",
        "\n",
        "# List to store sentiment scores and predicted labels\n",
        "tweet_scores = []\n",
        "\n",
        "# Example: Loop through each tweet, tokenize, get model output, and map to label\n",
        "for tweet in tweets_data:\n",
        "    # Tokenize the tweet text\n",
        "    encoded_tweet = tokenizer(tweet['text'], return_tensors='pt', padding=True, truncation=True)\n",
        "\n",
        "    # Pass the tokenized tweet to the model\n",
        "    output = model(**encoded_tweet)\n",
        "\n",
        "    # Get the logits from the model output\n",
        "    logits = output.logits\n",
        "\n",
        "    # Apply softmax to get probabilities\n",
        "    probabilities = F.softmax(logits, dim=-1)\n",
        "\n",
        "    # Get the predicted class (index of highest probability)\n",
        "    predicted_class_idx = torch.argmax(probabilities, dim=-1).item()\n",
        "\n",
        "    # Map the predicted class index to the corresponding label\n",
        "    predicted_label = labels[predicted_class_idx]\n",
        "\n",
        "    # Store tweet text, label, and probabilities (scores)\n",
        "    tweet_scores.append({\n",
        "        'tweet': tweet['text'],\n",
        "        'predicted_label': predicted_label,\n",
        "        'probabilities': probabilities.tolist()[0]  # Convert tensor to list\n",
        "    })\n",
        "\n",
        "# Print the sentiment scores for all tweets\n",
        "for tweet_info in tweet_scores:\n",
        "    print(f\"Tweet: {tweet_info['tweet']}\")\n",
        "    print(f\"Predicted Label: {tweet_info['predicted_label']}\")\n",
        "    print(f\"Sentiment Scores: {tweet_info['probabilities']}\")\n",
        "    print('-' * 50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pZP_ETgTw-m_",
        "outputId": "bf0aadff-48c7-4d13-946b-37120064958f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tweet: RT @DougWahl1: OUR HERO OF THE DAY IS...\n",
            "\n",
            "Mackenzie Scott, the ex-wife to Jeff Bezos. This week Elon Musk said Scott was destroying western…\n",
            "Predicted Label: Neutral\n",
            "Sentiment Scores: [0.054586898535490036, 0.8520929217338562, 0.09332011640071869]\n",
            "--------------------------------------------------\n",
            "Tweet: RT @RealAlexJones: Exclusive! Alex Jones Thanks Elon Musk For X Corp Entering Infowars Bankruptcy Case And Provides Key Update On Fake Auct…\n",
            "Predicted Label: Neutral\n",
            "Sentiment Scores: [0.10996474325656891, 0.8549953103065491, 0.03503996133804321]\n",
            "--------------------------------------------------\n",
            "Tweet: Musk says his tax cuts will be a HARDSHIP for many\n",
            "because of the programs he'll be cutting. Tell me, Elon\n",
            "Musk, will you be cutting anything that currently\n",
            "goes to Billionaires like yourself? 🤔 https://t.co/oItqMGlQVH https://t.co/BDs62Dt7xo\n",
            "Predicted Label: Neutral\n",
            "Sentiment Scores: [0.3563109040260315, 0.6113693714141846, 0.03231976926326752]\n",
            "--------------------------------------------------\n",
            "Tweet: RT @PiperK: Elon Musk posted a crazy video about how we have to go through an \"extinction event\" to become stronger.\n",
            "Predicted Label: Neutral\n",
            "Sentiment Scores: [0.46518373489379883, 0.5148904919624329, 0.019925834611058235]\n",
            "--------------------------------------------------\n",
            "Tweet: RT @felipeneto: \"A Janja na pode brigar com o Elon Musk pra não criar atrito diplomático\" - Bolsonaro.\n",
            "\n",
            "O mesmo:\n",
            "\n",
            "- Ameaçou Joe Biden com P…\n",
            "Predicted Label: Negative\n",
            "Sentiment Scores: [0.6007213592529297, 0.3850312829017639, 0.014247343875467777]\n",
            "--------------------------------------------------\n",
            "Tweet: RT @roxmo: Janja manda recado vulgar para Elon Musk e comete grosseria contra mulher da plateia https://t.co/xoyQqj2x0l // É uma dama...\n",
            "Predicted Label: Neutral\n",
            "Sentiment Scores: [0.14803437888622284, 0.8103552460670471, 0.041610367596149445]\n",
            "--------------------------------------------------\n",
            "Tweet: RT @LeaveDelaware: Delaware judges targeted Elon Musk’s pay package despite having shareholder approval.\n",
            "\n",
            "Delaware judges crippled the lega…\n",
            "Predicted Label: Neutral\n",
            "Sentiment Scores: [0.18790525197982788, 0.7868698835372925, 0.025224899873137474]\n",
            "--------------------------------------------------\n",
            "Tweet: RT @WatcherGuru: JUST IN: 🇺🇸 Vivek Ramaswamy says he and Elon Musk's Department of Government Efficiency (DOGE) will delete certain agencie…\n",
            "Predicted Label: Neutral\n",
            "Sentiment Scores: [0.31189876794815063, 0.6466241478919983, 0.041477080434560776]\n",
            "--------------------------------------------------\n",
            "Tweet: RT @fellaraktar: Reminder.\n",
            "\n",
            "Ukraine decides when it is time to talk.\n",
            "\n",
            "Not Olaf Scholz.\n",
            "Not Viktor Orban.\n",
            "Not Donald Trump.\n",
            "\n",
            "And definitely…\n",
            "Predicted Label: Neutral\n",
            "Sentiment Scores: [0.2511599659919739, 0.7008116245269775, 0.04802846908569336]\n",
            "--------------------------------------------------\n",
            "Tweet: RT @ludmilagrilo11: Janja, Elon Musk e a poderosa equipe de Trump https://t.co/sfrFv3HsxK\n",
            "Predicted Label: Neutral\n",
            "Sentiment Scores: [0.05679504945874214, 0.8983926177024841, 0.04481235146522522]\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    }
  ]
}