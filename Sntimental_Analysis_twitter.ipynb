{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "JCBily-gnqJh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b68a006d-fdf0-4b20-eb98-b10b127f0588"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.46.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.26.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.6)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tweepy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lIUjJQb9hs1O",
        "outputId": "533dfe27-c23c-4efc-c2e6-73cb49253077"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tweepy in /usr/local/lib/python3.10/dist-packages (4.14.0)\n",
            "Requirement already satisfied: oauthlib<4,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from tweepy) (3.2.2)\n",
            "Requirement already satisfied: requests<3,>=2.27.0 in /usr/local/lib/python3.10/dist-packages (from tweepy) (2.32.3)\n",
            "Requirement already satisfied: requests-oauthlib<2,>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from tweepy) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27.0->tweepy) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27.0->tweepy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27.0->tweepy) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27.0->tweepy) (2024.8.30)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "from scipy.special import softmax"
      ],
      "metadata": {
        "id": "fqVaykWeoNxK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tweepy\n",
        "import time\n",
        "\n",
        "# Set up keys\n",
        "\n",
        "\n",
        "# Authenticate\n",
        "client = tweepy.Client(bearer_token=bearer_token, wait_on_rate_limit=True)  # Added wait_on_rate_limit=True\n",
        "\n",
        "# Search recent tweets\n",
        "query = \"elon musk\"\n",
        "response = client.search_recent_tweets(query=query, max_results=10)\n",
        "\n",
        "# List to store tweet data\n",
        "tweets_data = []\n",
        "\n",
        "# Process each tweet\n",
        "for tweet in response.data:\n",
        "    tweet_info = {\n",
        "        \"id\": tweet.id,\n",
        "        \"text\": tweet.text,\n",
        "        \"created_at\": tweet.created_at,\n",
        "        \"author_id\": tweet.author_id\n",
        "    }\n",
        "    tweets_data.append(tweet_info)\n",
        "    time.sleep(1)  # Added a 1-second delay between tweets\n",
        "\n",
        "# Now `tweets_data` is a list containing the tweet information\n",
        "print(tweets_data)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2jIeTmxOnJWi",
        "outputId": "fe193e0f-05de-4932-ce2e-277254dac475"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tweepy.client:Rate limit exceeded. Sleeping for 628 seconds.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'id': 1858375647112130875, 'text': 'RT @Vivek4real_: üá∫üá∏ Elon Musk just recommended bitcoiner Howard Lutnic as US Treasury ü§Ø\\n\\nHe has 100‚Äôs of millions of dollars in #Bitcoin ht‚Ä¶', 'created_at': None, 'author_id': None}, {'id': 1858375646449377365, 'text': 'RT @OliLondonTV: Elon Musk debuts new look. ü§ì https://t.co/UzCS35Plhb', 'created_at': None, 'author_id': None}, {'id': 1858375644662341743, 'text': 'RT @HaberKuresi034: üõë Enes Kanter, Trump‚Äôƒ±n Ardƒ±ndan Bu Kez Elon Musk ile Bir Araya Geldi!\\n\\nüìå Ge√ßtiƒüimiz g√ºnlerde ABD Ba≈ükanƒ± Donald Trump‚Ä¶', 'created_at': None, 'author_id': None}, {'id': 1858375644138336601, 'text': 'RT @solclownworld: @ZssBecker $CLOWN https://t.co/qu4jKm6Wbn CA: DSEofUG1xUoq8hhzrixf2rjvsjGYuLan7jfsLyKsTRfi\\n\\nPopularized by frequent Elon‚Ä¶', 'created_at': None, 'author_id': None}, {'id': 1858375642410029299, 'text': 'elon musk will leave twitter before i do', 'created_at': None, 'author_id': None}, {'id': 1858375641436999970, 'text': '@mjfree Elon musk sold out America by buying the election', 'created_at': None, 'author_id': None}, {'id': 1858375640577155117, 'text': 'RT @Carmenbae90I: Elon Musk has altered the like button for the U.S. elections. Is this true? #Election2024 #Trump #Kamala #FightforAmerica‚Ä¶', 'created_at': None, 'author_id': None}, {'id': 1858375640237629664, 'text': 'RT @RawheaD: „Ç§„Éº„É≠„É≥ÂÄã‰∫∫„Å´ÊâπÂà§ÁöÑ„Åß„ÇÇSpaceX„ÇÑStalink„Å´„ÅØ„Åã„Å™„ÇäËÇØÂÆöÁöÑ„Å™„Ç¢„Ç´„Ååùïè„Å´„ÅØÂ§ö„ÅÑ„Åå„ÄÅÁßëÂ≠¶ÔºèÂ§©Êñá„ÅÆË¶≥ÁÇπ„Åã„Çâ„ÇÇStarlink„ÅØÂ£äÊªÖÁöÑ„Å™Áí∞Â¢ÉÁ†¥Â£ä„ÇíË°å„Å£„Å¶„Çã„ÄÇ‰∏ãÊâã„Åó„Åü„ÇâËøë„ÄÖÁâπÂÆö„ÅÆÁ®ÆÈ°û„ÅÆÈõªÊ≥¢ÊúõÈÅ†Èè°Ë¶≥Ê∏¨„Åå‰∏çÂèØËÉΩ„Å´„Å™„Çã„Åã„ÇÇ„Åó„Çå„Å™„ÅÑ„Å®„ÅÑ„ÅÜÁ†îÁ©∂ÁµêÊûúÔºö\\n\\nhttp‚Ä¶', 'created_at': None, 'author_id': None}, {'id': 1858375640124117177, 'text': 'RT @solclownworld: @ZssBecker $CLOWN https://t.co/qu4jKm6Wbn CA: DSEofUG1xUoq8hhzrixf2rjvsjGYuLan7jfsLyKsTRfi\\n\\nPopularized by frequent Elon‚Ä¶', 'created_at': None, 'author_id': None}, {'id': 1858375635321680356, 'text': 'RT @merry123459: The most disturbing part about Doug Fords Starlink deal, isn‚Äôt so much about Starlink as it about him doing business with‚Ä¶', 'created_at': None, 'author_id': None}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Key Changes:\n",
        "process_tweet Function: This function processes the tweet text by splitting the text into words and replacing:\n",
        "\n",
        "Mentions (starting with @) with @user.\n",
        "\n",
        "URLs (starting with http) with http.\n",
        "\n",
        "The words are then joined back into a string.\n",
        "\n",
        "Processing Tweets: After extracting each tweet's original text, I call\n",
        "process_tweet() to create a processed version of the tweet's text.\n",
        "\n",
        "Storing Processed Text: Both the original tweet text (text) and the processed text (processed_text) are stored in the tweets_data list.\n",
        "\n"
      ],
      "metadata": {
        "id": "Qf7acRIilMwZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#load the model and tokenizer\n",
        "roberta = 'cardiffnlp/twitter-roberta-base-sentiment'\n",
        "model = AutoModelForSequenceClassification.from_pretrained(roberta)\n",
        "tokenizer = AutoTokenizer.from_pretrained(roberta)"
      ],
      "metadata": {
        "id": "2xZlBplLs7rW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loop through each tweet and print only the 'text' field\n",
        "for tweet in tweets_data:\n",
        "    print(tweet['text'])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NZpyJZHwvihL",
        "outputId": "d4361227-ba48-4659-eb8a-3bd4433dc0b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "@gp_pulipaka Fantastic breakdown of the data science landscape! This visual really captures the breadth of skills and knowledge areas needed. A great guide for aspiring data scientists!\n",
            "Stay tuned to get more such scientific motivation &amp; the latest trends in AI, ML, Deep Learning, Data Science and Roboticsü§ù\n",
            " \n",
            "#MotivationalMonday #MondayMotivation #Inspiration #Motivation #ProductivityBoost #ScienceMotivation #TechInspiration #Innovation #FutureOfTech #wsro2024 https://t.co/lvEtvDiNtD\n",
            "RT @KrutikaKuppalli: Another bad idea: Amid ongoing and emerging infectious disease threats, nominating RFK and Bhattacharya‚Äîfigures who sp‚Ä¶\n",
            "RT @KirkDBorne: Learn #Mathematics for Data Science for FREE using these Courses: https://t.co/sd7TRR8Plc‚Ä¶ ‚Äî compiled by @tut_ml\n",
            "‚Äî‚Äî‚Äî‚Äî\n",
            "#Line‚Ä¶\n",
            "RT @flipsidecrypto: When blockchains grow, we all benefit. But they don't magically grow on their own.\n",
            "\n",
            "Over the years, through the painsta‚Ä¶\n",
            "RT @flipsidecrypto: When blockchains grow, we all benefit. But they don't magically grow on their own.\n",
            "\n",
            "Over the years, through the painsta‚Ä¶\n",
            "RT @flipsidecrypto: When blockchains grow, we all benefit. But they don't magically grow on their own.\n",
            "\n",
            "Over the years, through the painsta‚Ä¶\n",
            "@kevinnbass I AM SCIENCE. DON'T FORGET DATA TOO.\n",
            "RT @flipsidecrypto: When blockchains grow, we all benefit. But they don't magically grow on their own.\n",
            "\n",
            "Over the years, through the painsta‚Ä¶\n",
            "RT @flipsidecrypto: When blockchains grow, we all benefit. But they don't magically grow on their own.\n",
            "\n",
            "Over the years, through the painsta‚Ä¶\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Initialize tokenizer (assuming a pre-trained tokenizer like BERT)\n",
        "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Loop through each tweet, tokenize the text, and print the encoded version\n",
        "for tweet in tweets_data:\n",
        "    encoded_tweet = tokenizer(tweet['text'], return_tensors='pt')\n",
        "    print(encoded_tweet)\n",
        "    # Pass the tokenized tweet to the model\n",
        "    output = model(**encoded_tweet)\n",
        "\n",
        "    # Print the model output\n",
        "    print(\"The model output is:\",\" \",output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pQobzZbbvtl2",
        "outputId": "0ce9d515-bdd4-4c51-aed8-154ed019d0c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': tensor([[  101, 19387,  1030,  8788,  4213,  7317,  2487,  1024,  2256,  5394,\n",
            "          1997,  1996,  2154,  2003,  1012,  1012,  1012, 11407,  3660,  1010,\n",
            "          1996,  4654,  1011,  2564,  2000,  5076,  2022, 28370,  1012,  2023,\n",
            "          2733,  3449,  2239, 14163,  6711,  2056,  3660,  2001,  9846,  2530,\n",
            "          1529,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
            "The model output is:   SequenceClassifierOutput(loss=None, logits=tensor([[-1.1594,  1.5885, -0.6231]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "{'input_ids': tensor([[  101, 19387,  1030,  2613,  9453,  2595, 14339,  2229,  1024,  7262,\n",
            "           999,  4074,  3557,  4283,  3449,  2239, 14163,  6711,  2005,  1060,\n",
            "         13058,  5738, 18558,  9028,  2015, 10528,  2553,  1998,  3640,  3145,\n",
            "         10651,  2006,  8275,  8740,  6593,  1529,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
            "The model output is:   SequenceClassifierOutput(loss=None, logits=tensor([[-0.3721,  1.6788, -1.5158]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "{'input_ids': tensor([[  101, 14163,  6711,  2758,  2010,  4171,  7659,  2097,  2022,  1037,\n",
            "         26479,  2005,  2116,  2138,  1997,  1996,  3454,  2002,  1005,  2222,\n",
            "          2022,  6276,  1012,  2425,  2033,  1010,  3449,  2239, 14163,  6711,\n",
            "          1010,  2097,  2017,  2022,  6276,  2505,  2008,  2747,  3632,  2000,\n",
            "         22301,  2015,  2066,  4426,  1029,   100, 16770,  1024,  1013,  1013,\n",
            "          1056,  1012,  2522,  1013,  1051,  4183,  4160, 24798,  2140,  4160,\n",
            "          2615,  2232, 16770,  1024,  1013,  1013,  1056,  1012,  2522,  1013,\n",
            "          1038,  5104,  2575,  2475, 11927,  2581,  2595,  2080,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1]])}\n",
            "The model output is:   SequenceClassifierOutput(loss=None, logits=tensor([[ 0.5958,  1.1357, -1.8043]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "{'input_ids': tensor([[  101, 19387,  1030, 11939,  2243,  1024,  3449,  2239, 14163,  6711,\n",
            "          6866,  1037,  4689,  2678,  2055,  2129,  2057,  2031,  2000,  2175,\n",
            "          2083,  2019,  1000, 14446,  2724,  1000,  2000,  2468,  6428,  1012,\n",
            "           102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1]])}\n",
            "The model output is:   SequenceClassifierOutput(loss=None, logits=tensor([[ 0.9720,  1.0735, -2.1785]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "{'input_ids': tensor([[  101, 19387,  1030, 17095,  7159,  2080,  1024,  1000,  1037,  5553,\n",
            "          3900,  6583, 17491,  2063, 16908,  2906,  4012,  1051,  3449,  2239,\n",
            "         14163,  6711, 10975,  2050,  6583,  2080, 13675,  2401,  2099,  2012,\n",
            "         28414,  8041,  2080,  1000,  1011,  8945, 14881, 10464,  1012,  1051,\n",
            "          2033, 25855,  1024,  1011,  2572,  5243,  3597,  2226,  3533,  7226,\n",
            "          2368,  4012,  1052,  1529,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1]])}\n",
            "The model output is:   SequenceClassifierOutput(loss=None, logits=tensor([[ 1.3193,  0.8745, -2.4223]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "{'input_ids': tensor([[  101, 19387,  1030, 20996,  2595,  5302,  1024,  5553,  3900,  2158,\n",
            "          2850, 28667,  9365, 29364, 11498,  3449,  2239, 14163,  6711,  1041,\n",
            "         15699,  2063,  7977, 11610, 24528, 14163,  2140,  5886,  4830,  5127,\n",
            "          2401, 16770,  1024,  1013,  1013,  1056,  1012,  2522,  1013,  1060,\n",
            "          6977,  4160,  4160,  3501,  2475,  2595,  2692,  2140,  1013,  1013,\n",
            "          1041,  8529,  2050,  5477,  2050,  1012,  1012,  1012,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
            "The model output is:   SequenceClassifierOutput(loss=None, logits=tensor([[-0.1779,  1.5222, -1.4470]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "{'input_ids': tensor([[  101, 19387,  1030,  2681,  9247, 10830,  2890,  1024,  8452,  6794,\n",
            "          9416,  3449,  2239, 14163,  6711,  1521,  1055,  3477,  7427,  2750,\n",
            "          2383, 18668,  6226,  1012,  8452,  6794, 24433,  1996,  4190,  2050,\n",
            "          1529,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1]])}\n",
            "The model output is:   SequenceClassifierOutput(loss=None, logits=tensor([[ 0.1611,  1.5932, -1.8470]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "{'input_ids': tensor([[  101, 19387,  1030,  3422,  2121, 27390,  2226,  1024,  2074,  1999,\n",
            "          1024,   100,  6819,  3726,  2243, 14115, 26760, 24079,  2758,  2002,\n",
            "          1998,  3449,  2239, 14163,  6711,  1005,  1055,  2533,  1997,  2231,\n",
            "          8122,  1006,  3899,  2063,  1007,  2097,  3972, 12870,  3056,  2287,\n",
            "         12273,  2666,  1529,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
            "The model output is:   SequenceClassifierOutput(loss=None, logits=tensor([[ 0.4227,  1.1518, -1.5949]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "{'input_ids': tensor([[  101, 19387,  1030,  3062,  5400, 25509,  2906,  1024, 14764,  1012,\n",
            "          5924,  7288,  2043,  2009,  2003,  2051,  2000,  2831,  1012,  2025,\n",
            "         20514,  8040, 28094,  1012,  2025, 13489, 19607,  2319,  1012,  2025,\n",
            "          6221,  8398,  1012,  1998,  5791,  1529,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
            "The model output is:   SequenceClassifierOutput(loss=None, logits=tensor([[ 0.1579,  1.1841, -1.4964]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "{'input_ids': tensor([[  101, 19387,  1030, 11320, 22117, 11733, 16523, 22360, 14526,  1024,\n",
            "          5553,  3900,  1010,  3449,  2239, 14163,  6711,  1041,  1037, 17491,\n",
            "         27360,  2050,  1041, 15549,  5051,  2139,  8398, 16770,  1024,  1013,\n",
            "          1013,  1056,  1012,  2522,  1013, 16420, 12881,  2615,  2509,  7898,\n",
            "          2595,  2243,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
            "The model output is:   SequenceClassifierOutput(loss=None, logits=tensor([[-0.8979,  1.8633, -1.1348]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "labels = ['Negative', 'Neutral', 'Positive']"
      ],
      "metadata": {
        "id": "GJaQLB5atbnG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn import functional as F\n",
        "import torch\n",
        "\n",
        "\n",
        "# List to store sentiment scores and predicted labels\n",
        "tweet_scores = []\n",
        "\n",
        "# Example: Loop through each tweet, tokenize, get model output, and map to label\n",
        "for tweet in tweets_data:\n",
        "    # Tokenize the tweet text\n",
        "    encoded_tweet = tokenizer(tweet['text'], return_tensors='pt', padding=True, truncation=True)\n",
        "\n",
        "    # Pass the tokenized tweet to the model\n",
        "    output = model(**encoded_tweet)\n",
        "\n",
        "    # Get the logits from the model output\n",
        "    logits = output.logits\n",
        "\n",
        "    # Apply softmax to get probabilities\n",
        "    probabilities = F.softmax(logits, dim=-1)\n",
        "\n",
        "    # Get the predicted class (index of highest probability)\n",
        "    predicted_class_idx = torch.argmax(probabilities, dim=-1).item()\n",
        "\n",
        "    # Map the predicted class index to the corresponding label\n",
        "    predicted_label = labels[predicted_class_idx]\n",
        "\n",
        "    # Store tweet text, label, and probabilities (scores)\n",
        "    tweet_scores.append({\n",
        "        'tweet': tweet['text'],\n",
        "        'predicted_label': predicted_label,\n",
        "        'probabilities': probabilities.tolist()[0]  # Convert tensor to list\n",
        "    })\n",
        "\n",
        "# Print the sentiment scores for all tweets\n",
        "for tweet_info in tweet_scores:\n",
        "    print(f\"Tweet: {tweet_info['tweet']}\")\n",
        "    print(f\"Predicted Label: {tweet_info['predicted_label']}\")\n",
        "    print(f\"Sentiment Scores: {tweet_info['probabilities']}\")\n",
        "    print('-' * 50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pZP_ETgTw-m_",
        "outputId": "bf0aadff-48c7-4d13-946b-37120064958f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tweet: RT @DougWahl1: OUR HERO OF THE DAY IS...\n",
            "\n",
            "Mackenzie Scott, the ex-wife to Jeff Bezos. This week Elon Musk said Scott was destroying western‚Ä¶\n",
            "Predicted Label: Neutral\n",
            "Sentiment Scores: [0.054586898535490036, 0.8520929217338562, 0.09332011640071869]\n",
            "--------------------------------------------------\n",
            "Tweet: RT @RealAlexJones: Exclusive! Alex Jones Thanks Elon Musk For X Corp Entering Infowars Bankruptcy Case And Provides Key Update On Fake Auct‚Ä¶\n",
            "Predicted Label: Neutral\n",
            "Sentiment Scores: [0.10996474325656891, 0.8549953103065491, 0.03503996133804321]\n",
            "--------------------------------------------------\n",
            "Tweet: Musk says his tax cuts will be a HARDSHIP for many\n",
            "because of the programs he'll be cutting. Tell me, Elon\n",
            "Musk, will you be cutting anything that currently\n",
            "goes to Billionaires like yourself? ü§î https://t.co/oItqMGlQVH https://t.co/BDs62Dt7xo\n",
            "Predicted Label: Neutral\n",
            "Sentiment Scores: [0.3563109040260315, 0.6113693714141846, 0.03231976926326752]\n",
            "--------------------------------------------------\n",
            "Tweet: RT @PiperK: Elon Musk posted a crazy video about how we have to go through an \"extinction event\" to become stronger.\n",
            "Predicted Label: Neutral\n",
            "Sentiment Scores: [0.46518373489379883, 0.5148904919624329, 0.019925834611058235]\n",
            "--------------------------------------------------\n",
            "Tweet: RT @felipeneto: \"A Janja na pode brigar com o Elon Musk pra n√£o criar atrito diplom√°tico\" - Bolsonaro.\n",
            "\n",
            "O mesmo:\n",
            "\n",
            "- Amea√ßou Joe Biden com P‚Ä¶\n",
            "Predicted Label: Negative\n",
            "Sentiment Scores: [0.6007213592529297, 0.3850312829017639, 0.014247343875467777]\n",
            "--------------------------------------------------\n",
            "Tweet: RT @roxmo: Janja manda recado vulgar para Elon Musk e comete grosseria contra mulher da plateia https://t.co/xoyQqj2x0l // √â uma dama...\n",
            "Predicted Label: Neutral\n",
            "Sentiment Scores: [0.14803437888622284, 0.8103552460670471, 0.041610367596149445]\n",
            "--------------------------------------------------\n",
            "Tweet: RT @LeaveDelaware: Delaware judges targeted Elon Musk‚Äôs pay package despite having shareholder approval.\n",
            "\n",
            "Delaware judges crippled the lega‚Ä¶\n",
            "Predicted Label: Neutral\n",
            "Sentiment Scores: [0.18790525197982788, 0.7868698835372925, 0.025224899873137474]\n",
            "--------------------------------------------------\n",
            "Tweet: RT @WatcherGuru: JUST IN: üá∫üá∏ Vivek Ramaswamy says he and Elon Musk's Department of Government Efficiency (DOGE) will delete certain agencie‚Ä¶\n",
            "Predicted Label: Neutral\n",
            "Sentiment Scores: [0.31189876794815063, 0.6466241478919983, 0.041477080434560776]\n",
            "--------------------------------------------------\n",
            "Tweet: RT @fellaraktar: Reminder.\n",
            "\n",
            "Ukraine decides when it is time to talk.\n",
            "\n",
            "Not Olaf Scholz.\n",
            "Not Viktor Orban.\n",
            "Not Donald Trump.\n",
            "\n",
            "And definitely‚Ä¶\n",
            "Predicted Label: Neutral\n",
            "Sentiment Scores: [0.2511599659919739, 0.7008116245269775, 0.04802846908569336]\n",
            "--------------------------------------------------\n",
            "Tweet: RT @ludmilagrilo11: Janja, Elon Musk e a poderosa equipe de Trump https://t.co/sfrFv3HsxK\n",
            "Predicted Label: Neutral\n",
            "Sentiment Scores: [0.05679504945874214, 0.8983926177024841, 0.04481235146522522]\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    }
  ]
}